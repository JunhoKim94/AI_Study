{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "from scipy.special import expit\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPNN():\n",
    "    def __init__(self, etha, nodes, inputs, outputs):\n",
    "        self.etha = etha\n",
    "        self.hidden_nodes = nodes\n",
    "        self.input = np.array(inputs)\n",
    "        self.output = np.array(outputs)\n",
    "        self.NN_structure()\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        #return 1/(np.exp(-x)+1) -->소수점때문에 overflow 현상 일어나서 scipy 모듈로 대체\n",
    "        return expit(x)\n",
    "    \n",
    "    def sigmoid_prime(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "    \n",
    "    def NN_structure(self):\n",
    "        \n",
    "        bias = np.ones(shape=(len(self.input),1))\n",
    "        self.input = np.concatenate([self.input,bias],axis=1)\n",
    "        inputs_num = len(self.input[0])\n",
    "        outputs_num = len(self.output[0])\n",
    "        \n",
    "        #weight\n",
    "        self.w1 = np.random.rand(inputs_num,self.hidden_nodes)\n",
    "        self.w2 = np.random.rand(self.hidden_nodes+1,outputs_num)\n",
    "        \n",
    "        #Problem 3 weight\n",
    "        #self.w1 = np.array([[-0.089,0.098],[0.028,-0.07],[0.092,-0.01]])\n",
    "        #self.w2 = np.array([[0.056],[0.067],[0.016]])\n",
    "        \n",
    "    def run(self, iteration, verbose=False, Error_graph = False):\n",
    "        t= time.time()\n",
    "        y =[]\n",
    "        for j in range(1,iteration+1):\n",
    "            sum1 = 0\n",
    "            sum2 = 0\n",
    "            sum_Error = 0\n",
    "            for i in range(len(self.input)):\n",
    "                \n",
    "                #calculation until hidden node\n",
    "                net = np.matmul(self.input[i],self.w1)\n",
    "                h = self.sigmoid(net)\n",
    "                #add bias to hidden node\n",
    "                h = np.concatenate([h,np.ones(1).T])\n",
    "                \n",
    "                #calculation until output\n",
    "                o_net = np.matmul(h,self.w2)\n",
    "                o = self.sigmoid(o_net)\n",
    "\n",
    "                #Cost function\n",
    "                Error = 0.5*sum((o-self.output[i])*(o-self.output[i]))\n",
    "                \n",
    "                #delta weight\n",
    "                delta_w_2 = np.multiply((o-self.output[i]),self.sigmoid_prime(o))\n",
    "                delta_w_1 = np.matmul(self.w2,delta_w_2)\n",
    "                delta_w_1= self.sigmoid_prime(h[:-1])*delta_w_1[:-1]\n",
    "                delta_w_1 = self.etha*np.outer(self.input[i],delta_w_1)\n",
    "                delta_w_2 = self.etha*np.outer(h,delta_w_2)\n",
    "                #sum delta weights all cases\n",
    "                sum1 =sum1 - delta_w_1\n",
    "                sum2 =sum2 - delta_w_2\n",
    "                sum_Error += Error\n",
    "                \n",
    "                #print(net,h,o_net,o)\n",
    "                #print(delta_w_2,\"\\n\",delta_w_1)\n",
    "                \n",
    "            #update weight\n",
    "            self.w1 = self.w1 + sum1\n",
    "            self.w2 = self.w2 + sum2\n",
    "            \n",
    "            #current iteration, error, time\n",
    "            if (verbose == True):\n",
    "                if (j%5000) == 0 or (j == iteration):\n",
    "                    print(\"iteration: %d/%d, Error = %f, time: %f\"%(j,iteration,sum_Error,time.time()-t))\n",
    "            #Error - iteration graph\n",
    "            if Error_graph == True:\n",
    "                y.append(sum_Error)\n",
    "        if Error_graph == True:\n",
    "            plt.plot(np.linspace(1,iteration,iteration),y,'black')\n",
    "            plt.title(\"Error by iteration\",size=20)\n",
    "            plt.xlabel('Iteration',size=20)\n",
    "            plt.ylabel('Error',size=20)\n",
    "            plt.show()\n",
    "    def feed_forward(self,x):\n",
    "        bias = np.ones(shape=(1))\n",
    "        x = np.concatenate([x,bias])\n",
    "        net = np.matmul(x,self.w1)\n",
    "        h = self.sigmoid(net)\n",
    "        h = np.concatenate([h,np.ones(1).T])    \n",
    "        o_net = np.matmul(h,self.w2)\n",
    "        o = self.sigmoid(o_net)\n",
    "        return o\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "problem 3\n",
    "a = [[1,1],[1,0],[0,1],[0,0]]\n",
    "b= [[0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]]\n",
    "bp = BPNN(etha = 0.5 , nodes= 2 , inputs=a, outputs=b)\n",
    "bp.run(2,verbose = False,Error_graph = False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 5000/500000, Error = 0.042832, time: 3.318131\n",
      "iteration: 10000/500000, Error = 0.042822, time: 6.843736\n",
      "iteration: 15000/500000, Error = 0.042870, time: 10.602680\n",
      "iteration: 20000/500000, Error = 0.042883, time: 14.331719\n",
      "iteration: 25000/500000, Error = 0.042835, time: 17.823507\n",
      "iteration: 30000/500000, Error = 0.042666, time: 21.150568\n",
      "iteration: 35000/500000, Error = 0.042730, time: 24.664172\n",
      "iteration: 40000/500000, Error = 0.042515, time: 28.288516\n",
      "iteration: 45000/500000, Error = 0.040696, time: 31.617615\n",
      "iteration: 50000/500000, Error = 0.039744, time: 34.890828\n",
      "iteration: 55000/500000, Error = 0.059063, time: 38.216972\n",
      "iteration: 60000/500000, Error = 0.023592, time: 41.610892\n",
      "iteration: 65000/500000, Error = 0.005552, time: 44.963931\n",
      "iteration: 70000/500000, Error = 0.025599, time: 48.342856\n",
      "iteration: 75000/500000, Error = 0.022150, time: 51.641037\n",
      "iteration: 80000/500000, Error = 0.016332, time: 54.937257\n",
      "iteration: 85000/500000, Error = 0.010787, time: 58.226467\n",
      "iteration: 90000/500000, Error = 0.008159, time: 61.490732\n",
      "iteration: 95000/500000, Error = 0.012924, time: 64.807866\n",
      "iteration: 100000/500000, Error = 0.009466, time: 68.093079\n",
      "iteration: 105000/500000, Error = 0.009226, time: 71.393249\n",
      "iteration: 110000/500000, Error = 0.010821, time: 74.664502\n",
      "iteration: 115000/500000, Error = 0.006956, time: 77.909830\n",
      "iteration: 120000/500000, Error = 0.004830, time: 81.171069\n",
      "iteration: 125000/500000, Error = 0.003579, time: 84.448347\n",
      "iteration: 130000/500000, Error = 0.002751, time: 87.734517\n",
      "iteration: 135000/500000, Error = 0.002521, time: 90.997830\n",
      "iteration: 140000/500000, Error = 0.002419, time: 94.292027\n",
      "iteration: 145000/500000, Error = 0.004336, time: 97.582226\n",
      "iteration: 150000/500000, Error = 0.004365, time: 100.863413\n",
      "iteration: 155000/500000, Error = 0.004310, time: 104.184533\n",
      "iteration: 160000/500000, Error = 0.004219, time: 107.502693\n",
      "iteration: 165000/500000, Error = 0.004117, time: 110.800879\n",
      "iteration: 170000/500000, Error = 0.004022, time: 114.536849\n",
      "iteration: 175000/500000, Error = 0.003943, time: 118.515247\n",
      "iteration: 180000/500000, Error = 0.003886, time: 122.505538\n",
      "iteration: 185000/500000, Error = 0.003856, time: 125.805717\n",
      "iteration: 190000/500000, Error = 0.003851, time: 129.070026\n",
      "iteration: 195000/500000, Error = 0.003868, time: 132.327316\n",
      "iteration: 200000/500000, Error = 0.003901, time: 135.612531\n",
      "iteration: 205000/500000, Error = 0.003944, time: 138.879756\n",
      "iteration: 210000/500000, Error = 0.003989, time: 142.152046\n",
      "iteration: 215000/500000, Error = 0.004031, time: 145.425291\n",
      "iteration: 220000/500000, Error = 0.004068, time: 148.731408\n",
      "iteration: 225000/500000, Error = 0.004096, time: 152.557181\n",
      "iteration: 230000/500000, Error = 0.004115, time: 156.169559\n",
      "iteration: 235000/500000, Error = 0.004125, time: 159.661226\n",
      "iteration: 240000/500000, Error = 0.004129, time: 162.923501\n",
      "iteration: 245000/500000, Error = 0.004126, time: 166.189768\n",
      "iteration: 250000/500000, Error = 0.004118, time: 169.501906\n",
      "iteration: 255000/500000, Error = 0.004105, time: 172.791075\n",
      "iteration: 260000/500000, Error = 0.004090, time: 176.042419\n",
      "iteration: 265000/500000, Error = 0.004071, time: 179.306693\n",
      "iteration: 270000/500000, Error = 0.004051, time: 182.563976\n",
      "iteration: 275000/500000, Error = 0.004029, time: 185.846163\n",
      "iteration: 280000/500000, Error = 0.004006, time: 189.144388\n",
      "iteration: 285000/500000, Error = 0.003981, time: 192.419626\n",
      "iteration: 290000/500000, Error = 0.003956, time: 195.699855\n",
      "iteration: 295000/500000, Error = 0.003931, time: 199.015945\n",
      "iteration: 300000/500000, Error = 0.003904, time: 202.609373\n",
      "iteration: 305000/500000, Error = 0.003878, time: 205.930492\n",
      "iteration: 310000/500000, Error = 0.003851, time: 209.185753\n",
      "iteration: 315000/500000, Error = 0.003824, time: 212.446069\n",
      "iteration: 320000/500000, Error = 0.003797, time: 215.705360\n",
      "iteration: 325000/500000, Error = 0.003770, time: 218.996553\n",
      "iteration: 330000/500000, Error = 0.003742, time: 222.237888\n",
      "iteration: 335000/500000, Error = 0.003715, time: 225.495183\n",
      "iteration: 340000/500000, Error = 0.003688, time: 228.786379\n",
      "iteration: 345000/500000, Error = 0.003660, time: 232.057627\n",
      "iteration: 350000/500000, Error = 0.003633, time: 235.439546\n",
      "iteration: 355000/500000, Error = 0.003606, time: 238.709843\n",
      "iteration: 360000/500000, Error = 0.003578, time: 242.004993\n",
      "iteration: 365000/500000, Error = 0.003551, time: 245.263319\n",
      "iteration: 370000/500000, Error = 0.003524, time: 248.579451\n",
      "iteration: 375000/500000, Error = 0.003497, time: 251.929488\n",
      "iteration: 380000/500000, Error = 0.003470, time: 255.201739\n",
      "iteration: 385000/500000, Error = 0.003444, time: 258.532838\n",
      "iteration: 390000/500000, Error = 0.003417, time: 261.826031\n",
      "iteration: 395000/500000, Error = 0.003391, time: 265.110206\n",
      "iteration: 400000/500000, Error = 0.003364, time: 268.415367\n",
      "iteration: 405000/500000, Error = 0.003338, time: 271.711553\n",
      "iteration: 410000/500000, Error = 0.003312, time: 274.986831\n",
      "iteration: 415000/500000, Error = 0.003287, time: 278.262073\n",
      "iteration: 420000/500000, Error = 0.003261, time: 281.553236\n",
      "iteration: 425000/500000, Error = 0.003236, time: 284.844484\n",
      "iteration: 430000/500000, Error = 0.003211, time: 288.167549\n",
      "iteration: 435000/500000, Error = 0.003186, time: 292.492982\n",
      "iteration: 440000/500000, Error = 0.003161, time: 296.134287\n",
      "iteration: 445000/500000, Error = 0.003137, time: 299.437455\n",
      "iteration: 450000/500000, Error = 0.003113, time: 302.734600\n",
      "iteration: 455000/500000, Error = 0.003089, time: 306.009841\n",
      "iteration: 460000/500000, Error = 0.003065, time: 309.261180\n",
      "iteration: 465000/500000, Error = 0.003042, time: 312.528448\n",
      "iteration: 470000/500000, Error = 0.003018, time: 315.846575\n",
      "iteration: 475000/500000, Error = 0.002995, time: 319.157683\n",
      "iteration: 480000/500000, Error = 0.002973, time: 322.502780\n",
      "iteration: 485000/500000, Error = 0.002950, time: 325.799959\n",
      "iteration: 490000/500000, Error = 0.002928, time: 329.154988\n",
      "iteration: 495000/500000, Error = 0.002906, time: 332.436210\n",
      "iteration: 500000/500000, Error = 0.002885, time: 335.733359\n",
      "[[-7.47988215 -1.99361267 -1.63566339 -3.44533889]\n",
      " [ 7.4514876  -2.43646043 -2.4235325  -2.7121973 ]]\n",
      "[[  7.68698188]\n",
      " [-13.0959445 ]\n",
      " [ -7.31426573]\n",
      " [-30.98205344]\n",
      " [ -7.56088407]]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #make a target & input\n",
    "    x = np.arange(0,1.05,0.1).reshape(11,1)\n",
    "    target = f(x)\n",
    "    #NN create\n",
    "    quadratic = BPNN(0.7, 4, x, target)\n",
    "    \n",
    "    #NN run\n",
    "    quadratic.run(500000, verbose =True, Error_graph = True)\n",
    "    \n",
    "    #x for graph\n",
    "    a = np.linspace(0,1,100)\n",
    "    x = []\n",
    "    for i in a:\n",
    "        x.append(ap.feed_forward([i])[0])\n",
    "    #Graph \n",
    "    figure = plt.figure()\n",
    "    plt.plot(a,x,'r')\n",
    "    plt.plot(np.linspace(0,1,11),target,'b')\n",
    "    plt.title(\"Result\",size=20)\n",
    "    plt.legend(['train','target'],loc=2,prop={'size': 20})\n",
    "    plt.xlabel('Input',size=20)\n",
    "    plt.ylabel('Output',size=20)\n",
    "    plt.show()\n",
    "    #Print final weight\n",
    "    print(ap.w1)\n",
    "    print(ap.w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
